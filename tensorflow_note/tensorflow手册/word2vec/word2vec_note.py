'''
学习文字的向量表示 word embedding
'''
'''
为什么需要学习Word Embeddings?
  通常图像或音频系统处理的是由图片中所有单个原始像素点强度值或音频中功率谱密度的强度值，把它们
  编码成丰富 高纬度的向量数据集。 
  自然语言处理系统通常将词汇作为离散的单一符号，例如 "cat" 一词或可表示为 Id537 ，
  而 "dog" 一词或可表示为 Id143。
  可见，将词汇表达为上述的独立离散符号将进一步导致数据稀疏，使我们在训练统计模型时不得不寻求更多的数据。
  而词汇的向量表示将克服上述的难题

  向量空间模型(VSMs)将词汇表达嵌套于一个连续的向量空间中，语义近似的词汇被映射为相邻的数据点。
  向量空间模型在自然语言处理领域中有很长的历史，不过一般都依赖于distributional Hypothesis(分布式假设)
  其核心思想是出现于上下文情景中的词汇都有相类似的语义。

  采用这一假设的研究方法大致分为两类:
    基于计数的方法(潜在语义分析)  和 预测方法(神经概率化语言模型)
    基于计数的方法计算某词汇与其邻近词汇在一个大型语料库中共同出现的频率及其他统计量，
        然后将这些统计量映射到一个小型且稠密的向量中。
    预测方法则试图直接从某词汇的邻近词汇对其进行预测，在此过程中利用以及学习到的小型
        且稠密的嵌套向量。
'''
'''
Word2vec 是一种可以进行高效率词嵌套学习的预测模型。其两种变体分别为 连续词袋模型(CBOW)及
Skip-Gram模型。
CBOW 根据源词上下文词汇('the cat sits on the')来预测目标词汇(例如'mat')，
Skip-Gram相反，它通过目标词汇来预测源词汇。采取CBOW的逆过程动机为
  CBOW算法对于很多分布式信息进行了平滑处理(例如将一整段上下文信息视为一个单一观察量)对于小型的数据集有效
  Skip-Gram模型将每个'上下文-目标词汇'的组合视为一个新观察量，在大型数据集中会更为有效。
'''

